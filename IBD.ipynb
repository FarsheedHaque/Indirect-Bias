{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QZFQPGYciCp4",
    "outputId": "4d91ef79-210f-4ce6-feef-9e838ad65d62"
   },
   "outputs": [],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ajhlNraaf38f"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import DistilBertTokenizer,DistilBertForSequenceClassification,DistilBertConfig\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.spatial.distance import euclidean as euclidean_dist\n",
    "from scipy.spatial.distance import cosine as cosine_sim\n",
    "from scipy.integrate import trapz\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SqeIFlWLVOqO"
   },
   "outputs": [],
   "source": [
    "def input_tokenizer(tokenizer, text, leng):\n",
    "  input_tokens = []\n",
    "  input_ids = []\n",
    "  attention_mask = []\n",
    "\n",
    "  for doc in text:\n",
    "    tkn = tokenizer.encode_plus(doc, add_special_tokens=True, truncation = True, max_length = leng, return_tensors='pt', return_attention_mask=True)\n",
    "    id = tkn['input_ids']\n",
    "    mask = tkn['attention_mask']\n",
    "\n",
    "    input_tokens.append(tkn)\n",
    "    input_ids.append(id)\n",
    "    attention_mask.append(mask)\n",
    "\n",
    "  return input_tokens, input_ids, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Lpqk3Q2fVIb"
   },
   "outputs": [],
   "source": [
    "def interpretation(tokenizer, input_ids, attention_weights):\n",
    "  importance_scores = []\n",
    "\n",
    "  for attention in attention_weights:\n",
    "    attention = torch.sum(attention, dim=1)  \n",
    "    attention = attention.squeeze().detach().numpy()\n",
    "\n",
    "    valid_tokens = tokenizer.convert_ids_to_tokens(input_ids.squeeze())[1:-1]\n",
    "    valid_attention_weights = attention[1:-1, 1:-1]\n",
    "\n",
    "    importance = np.sum(valid_attention_weights, axis=0)\n",
    "    importance_scores.append(importance)\n",
    "\n",
    "  importance_scores = np.sum(importance_scores, axis=0)\n",
    "\n",
    "  return valid_tokens, importance_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5SuTMW4iejIS"
   },
   "outputs": [],
   "source": [
    "def evaluation(tokenizer, model, input_ids, attention_mask):\n",
    "  pred = []\n",
    "  valid_tokens = []\n",
    "  importance_scores = []\n",
    "\n",
    "  for i in range(len(input_ids)):\n",
    "    with torch.no_grad():\n",
    "      outputs = model(input_ids[i], attention_mask=attention_mask[i], output_attentions=True)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    attention_weights = outputs.attentions\n",
    "\n",
    "    # Apply softmax and get predictions\n",
    "    probabilities = torch.softmax(logits, dim=1)\n",
    "    predicted_class = torch.argmax(probabilities, dim=1).item()\n",
    "\n",
    "    token, score = interpretation(tokenizer, input_ids[i], attention_weights)\n",
    "\n",
    "    valid_tokens.append(token)\n",
    "    importance_scores.append(score)\n",
    "    pred.append(predicted_class)\n",
    "\n",
    "  return pred, valid_tokens, importance_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha_filter(token, score_y, score_s):\n",
    "\n",
    "    new_tokens = []\n",
    "    new_scores_y = []\n",
    "    new_scores_s = []\n",
    "\n",
    "    for i in range(len(valid_tokens_y)):\n",
    "        filtered_tokens = []\n",
    "        filtered_scores_y = []\n",
    "        filtered_scores_s = []\n",
    "\n",
    "        for t, y, s in zip(valid_tokens_y[i], importance_scores_y[i], importance_scores_s[i]):\n",
    "            if t.isalpha():\n",
    "                filtered_tokens.append(t)\n",
    "                filtered_scores_y.append(y)\n",
    "                filtered_scores_s.append(s)\n",
    "                \n",
    "        new_tokens.append(filtered_tokens)\n",
    "        new_scores_y.append(filtered_scores_y)\n",
    "        new_scores_s.append(filtered_scores_s)\n",
    "    \n",
    "    return new_tokens, new_scores_y, new_scores_s\n",
    "\n",
    "\n",
    "def topK_extractor(token, score, percentile):\n",
    "    \n",
    "    t_new = []\n",
    "    s_new = []\n",
    "\n",
    "    for t,s in zip(token, score):\n",
    "\n",
    "        sorted_scores = sorted(s, reverse=True)\n",
    "        threshold_index = int(len(sorted_scores) * percentile / 100)\n",
    "\n",
    "        threshold = sorted_scores[threshold_index-1]\n",
    "        s_filtered = [x for x in s if x >= threshold]\n",
    "        t_filtered = [t[i] for i, x in enumerate(s) if x >= threshold]\n",
    "        s_new.append(s_filtered)\n",
    "        t_new.append(t_filtered)\n",
    "\n",
    "    return t_new, s_new\n",
    "\n",
    "\n",
    "def jaccard_similarity(arr1, arr2):\n",
    "    sim = []\n",
    "    \n",
    "    for i in range(len(arr1)):\n",
    "        set_a1 = set(arr1[i])\n",
    "        set_a2 = set(arr2[i])\n",
    "        intersection = set_a1.intersection(set_a2)\n",
    "        union = set_a1.union(set_a2)\n",
    "        similarity = len(intersection) / len(union)\n",
    "        sim.append(similarity)\n",
    "\n",
    "    return sim, np.mean(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UCvsUo5sLI4I"
   },
   "outputs": [],
   "source": [
    "def demographic_parity(label, sensitive_att):\n",
    "\n",
    "  m1,m0,f1,f0 = 0,0,0,0\n",
    "\n",
    "  for i in range(len(label)):\n",
    "    if label[i] == 1 and sensitive_att[i]== 1:\n",
    "      f1 = f1 +1\n",
    "\n",
    "    if label[i] == 0 and sensitive_att[i]== 1:\n",
    "      f0 = f0 +1\n",
    "\n",
    "    if label[i] == 1 and sensitive_att[i]== 0:\n",
    "      m1 = m1 +1\n",
    "\n",
    "    if label[i] == 0 and sensitive_att[i]== 0:\n",
    "      m0 = m0 +1\n",
    "\n",
    "  rd = abs((m1/(m1+m0)) - (f1/(f1+f0)))\n",
    "\n",
    "  return rd\n",
    "\n",
    "\n",
    "def euqal(label_flat, pred_flat, test_gender):\n",
    "    m_true = []\n",
    "    m_pred = []\n",
    "    f_true = []\n",
    "    f_pred = []\n",
    "\n",
    "    for i in range(len(test_gender)):\n",
    "      if(test_gender[i] == 0):\n",
    "        m_true.append(label_flat[i])\n",
    "        m_pred.append(pred_flat[i])\n",
    "      else:\n",
    "        f_true.append(label_flat[i])\n",
    "        f_pred.append(pred_flat[i])\n",
    "\n",
    "    tn_m, fp_m, fn_m, tp_m = confusion_matrix(m_true, m_pred).ravel()\n",
    "    tn_f, fp_f, fn_f, tp_f = confusion_matrix(f_true, f_pred).ravel()\n",
    "\n",
    "    TPR_m = tp_m / (tp_m + fn_m)\n",
    "    FPR_m = fp_m / (fp_m + tn_m)\n",
    "\n",
    "    TPR_f = tp_f / (tp_f + fn_f)\n",
    "    FPR_f = fp_f / (fp_f + tn_f)\n",
    "\n",
    "    equality_of_odds = abs(TPR_m - TPR_f) + abs(FPR_m - FPR_f)\n",
    "    equality_of_opportunity = abs(TPR_m - TPR_f)\n",
    "\n",
    "    return equality_of_odds, equality_of_opportunity\n",
    "\n",
    "\n",
    "def metrics(label, pred):\n",
    "  accuracy = accuracy_score(label, pred)\n",
    "  f1_micro = f1_score(label, pred, average=\"micro\")\n",
    "  f1_macro = f1_score(label, pred, average=\"macro\")\n",
    "  f1_weighted = f1_score(label, pred, average=\"weighted\")\n",
    "  tn, fp, fn, tp = confusion_matrix(label, pred).ravel()\n",
    "  precision = tp / (tp + fp)\n",
    "  recall = tp / (tp + fn)\n",
    "\n",
    "  return accuracy, f1_micro, f1_macro, f1_weighted, tn, fp, fn, tp, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rv8q8R4kkfon"
   },
   "outputs": [],
   "source": [
    "dataset_dir = 'toxic/test.csv'\n",
    "model_y_dir = 'toxic/models/bert/adhe/model'\n",
    "model_s_dir = 'toxic/models/old/dp-20/model_toxic_s'\n",
    "leng = 128\n",
    "\n",
    "main_df = pd.read_csv(filepath_or_buffer=dataset_dir)\n",
    "text = main_df.text.values.tolist()\n",
    "y = main_df.label.values.tolist()\n",
    "s = main_df.gender.values.tolist()\n",
    "\n",
    "tokenizer_y = BertTokenizer.from_pretrained(model_y_dir)\n",
    "model_y = BertForSequenceClassification.from_pretrained(model_y_dir, num_labels = 2, output_attentions=True)\n",
    "\n",
    "tokenizer_s = BertTokenizer.from_pretrained(model_s_dir)\n",
    "model_s = BertForSequenceClassification.from_pretrained(model_s_dir, num_labels = 2, output_attentions=True)\n",
    "\n",
    "input_tokens_y, input_ids_y, attention_mask_y = input_tokenizer(tokenizer_y, text, leng)\n",
    "input_tokens_s, input_ids_s, attention_mask_s = input_tokenizer(tokenizer_s, text, leng)\n",
    "\n",
    "pred_y, valid_tokens_y, importance_scores_y = evaluation(tokenizer_y, model_y, input_ids_y, attention_mask_y)\n",
    "pred_s, valid_tokens_s, importance_scores_s = evaluation(tokenizer_s, model_s, input_ids_s, attention_mask_s)\n",
    "\n",
    "accuracy_y, f1_micro_y, f1_macro_y, f1_weighted_y, tn_y, fp_y, fn_y, tp_y, precision_y, recall_y = metrics(y, pred_y)\n",
    "accuracy_s, f1_micro_s, f1_macro_s, f1_weighted_s, tn_s, fp_s, fn_s, tp_s, precision_s, recall_s = metrics(s, pred_s)\n",
    "rd_true = demographic_parity(y, s)\n",
    "rd_pred = demographic_parity(pred_y, s)\n",
    "odds, opp = euqal(y, pred_y, s)\n",
    "\n",
    "print(\"For model Y\")\n",
    "print(\"accuracy: %.6f\"%accuracy_y)\n",
    "print(\"f1_micro: %.6f, f1_macro: %.3f, f1_weighted: %.3f\"%(f1_micro_y, f1_macro_y, f1_weighted_y))\n",
    "print(\"precision: %.6f\"%precision_y)\n",
    "print(\"recall: %.6f\"%recall_y)\n",
    "print(\"tp:\",tp_y, \"fp:\",fp_y, \"tn:\",tn_y, \"fn:\",fn_y)\n",
    "\n",
    "print(\"\\nFor model S\")\n",
    "print(\"accuracy: %.6f\"%accuracy_s)\n",
    "print(\"f1_micro: %.6f, f1_macro: %.3f, f1_weighted: %.3f\"%(f1_micro_s, f1_macro_s, f1_weighted_s))\n",
    "print(\"precision: %.6f\"%precision_s)\n",
    "print(\"recall: %.6f\"%recall_s)\n",
    "print(\"tp:\",tp_s, \"fp:\",fp_s, \"tn:\",tn_s, \"fn:\",fn_s)\n",
    "\n",
    "print(\"\\nFairness Metrics\")\n",
    "print(\"\\nDemographic Parity\")\n",
    "print(\"Before prediction: %.6f\"%rd_true)\n",
    "print(\"After prediction: %.6f\"%rd_pred)\n",
    "\n",
    "print(\"\\nEqualized Odds: %.6f\"%odds)\n",
    "print(\"Equal Opportunity: %.6f\"%opp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_tokens, importance_scores_y, importance_scores_s = alpha_filter(valid_tokens_y, importance_scores_y, importance_scores_s)\n",
    "\n",
    "top = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\n",
    "\n",
    "jaccard_ty = []\n",
    "jaccard_ts = []\n",
    "\n",
    "for i in top:\n",
    "    jty, jy = topK_extractor(valid_tokens, importance_scores_y, i*100)\n",
    "    jts, js = topK_extractor(valid_tokens, importance_scores_s, i*100)\n",
    "    jaccard_ty.append(jty)\n",
    "    jaccard_ts.append(jts)\n",
    "    \n",
    "top.insert(0, 0.0) \n",
    "\n",
    "jaccard = [0]\n",
    "\n",
    "for i in range(len(jaccard_ty)):\n",
    "    j, j_avg = jaccard_similarity(jaccard_ty[i], jaccard_ts[i])\n",
    "    jaccard.append(j_avg)\n",
    "    \n",
    "print(jaccard)\n",
    "print(\"AUSC:\", trapz(np.array(jaccard), top))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
